<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Mozilla on Il y a du thé renversé au bord de la table !</title>
    <link>https://yoric.github.io/categories/mozilla/index.xml</link>
    <description>Recent content in Mozilla on Il y a du thé renversé au bord de la table !</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://yoric.github.io/categories/mozilla/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Things to do before you[r project] die[s]</title>
      <link>https://yoric.github.io/post/how-to-survive/</link>
      <pubDate>Fri, 17 Feb 2017 00:10:34 +0100</pubDate>
      
      <guid>https://yoric.github.io/post/how-to-survive/</guid>
      <description>

&lt;p&gt;It&amp;rsquo;s no secret that all projects die, eventually, and that most projects die
before they have had a chance to have any impact. This is true of personal projects,
of open-source projects, of startup projects, of hardware projects, and more.&lt;/p&gt;

&lt;p&gt;A few days ago, &lt;a href=&#34;https://wiki.mozilla.org/Connected_Devices/Projects/Project_Lighthouse&#34;&gt;the IoT project on which I was working&lt;/a&gt;
did just that. It was funded, it was open-source, it was open-hardware, and it
still died before having the opportunity to release anything useful. In our case,
we got pretty far, didn&amp;rsquo;t run out of funds, and were a little polish away from
putting our first alpha-testers in front of a pretty advanced prototype before
a higher-level strategy pivot killed a number of projects at once.&lt;/p&gt;

&lt;p&gt;Ah, well, risks of the trade.&lt;/p&gt;

&lt;p&gt;Still, working on it, we managed to learn or come up with a few tricks
to increase the chances of surviving at least long enough to release a product.
I&amp;rsquo;ll try to summarize and explain some of these tricks here. Hopefully, if you
have a project, regardless of funding and medium, you may find some of this
summary useful.&lt;/p&gt;

&lt;h1 id=&#34;death-by-nobody-will-use-your-project&#34;&gt;Death by Nobody-will-use-your-project&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;tl;dr&lt;/p&gt;

&lt;p&gt;There are many good ways to test an idea way before the prototype is solid
enough to show it up. Use them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You have an idea. It&amp;rsquo;s certainly a great idea. Your friends and your family
probably love it. A few geeks around you are enthusiastic, it can change the world!&lt;/p&gt;

&lt;p&gt;At this stage, you are eager to start coding/designing/burning circuits/&amp;hellip;
And that&amp;rsquo;s great. By all means, if you&amp;rsquo;re enjoying yourself, please carry on.
However, before you start committing to the project anything you might regret –
leaving your day job, spending your money or your investors&amp;rsquo; – you should make
sure that your project has a chance to gain users and traction.&lt;/p&gt;

&lt;p&gt;If you come from a technological background, you may believe that you need to
put your project in front of users to get feedback, which means that you need
a pretty advanced prototype. &lt;em&gt;If you do this, you considerably increase your
chances of your project dying on you.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So time to put your Survivor Hat.&lt;/p&gt;

&lt;p&gt;There are many ways to get user feedback while the project is in its
infancy. Doing this used to be long, expensive and – just as important for an indie
project – boring. These days, there are a few services that do it for you
and actually bring money to your project in case of success. Yes, I am talking of &lt;a href=&#34;http://kickstarter.com/&#34;&gt;Kickstarter&lt;/a&gt;, &lt;a href=&#34;http://indiegogo.com/&#34;&gt;Indiegogo&lt;/a&gt;
&amp;amp; &lt;a href=&#34;https://en.wikipedia.org/wiki/Comparison_of_crowdfunding_services&#34;&gt;co&lt;/a&gt;. If you
believed that these crowdfunding platforms were about gathering money,
welcome to the worst-kept secret of the 2010s:
they are actually marketing platforms that will let you find out &lt;em&gt;if people are
willing to spend money on your project&lt;/em&gt;. Which, in many cases, is a pretty good
measure of whether you should greenlight your own project. Just be sure to be
honest regarding the current state of your project and how the money will be
used. If you lie, even with the best of intentions, your funders will eventually
find out and they will be legitimately angry.&lt;/p&gt;

&lt;p&gt;An alternative, depending on what you&amp;rsquo;re doing, is to open a website and start
taking pre-orders (morally dubious), start letting interested parties subscribe
to an announcement mailing-list (better), or get creative. I have even seen weirder variants in
which non-technical founders started a fairly sophisticated online service
before they even hired a single developer – by hiring underpaid students to actually do
the work of the machine, at least long enough to determine whether users would
actually use them (neat trick, morally&amp;hellip; weird). I have also seen at least one startup-turned-internet-giant who implemented only a subset of the promised
features and measured how many users clicked on buttons that were not
implemented yet (not a trick I like, but it worked for them).&lt;/p&gt;

&lt;p&gt;Note that all these techniques help you determine your potential end users.
Unless your end users are developers, that&amp;rsquo;s entirely orthogonal to getting
people to take part in the development of your project.&lt;/p&gt;

&lt;p&gt;This list is clearly not exhaustive. Now that you have your Survivor Hat on,
you should be able to come up alternatives if you need to.&lt;/p&gt;

&lt;p&gt;Regardless of the technique you use, spending a few days of work to measuring
your potential audience can save you from disastrous choices for your career
(or savings, or startup, &amp;hellip;). As a bonus, putting together some communication
will also serve to put your own thoughts in order, decide of your priorities,
get feedback from your future users. Also, if you need money, it can serve to
fund you (crowdfunding) and/or to convince investors.&lt;/p&gt;

&lt;h1 id=&#34;death-by-tests-need-users-need-tests&#34;&gt;Death by Tests-need-users-need-tests&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;tl;dr&lt;/p&gt;

&lt;p&gt;There are many good ways to test algorithms, UX, accessibility, &amp;hellip; before the
prototype is solid enough to show it up. Use them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At this stage, I&amp;rsquo;m assuming that you have identified your user population and
the conditions in which your project will be used. You will, of course, need
to adapt – you will need different testers for an OCR-for-the-blind or for a
superbowl-adblocker. For narrative purposes, though, I&amp;rsquo;ll pretend that your testers
somehow lurk in your hallway.&lt;/p&gt;

&lt;p&gt;So, your users are interested, your project is going full speed ahead. Great.
But, like it or not, full speed ahead is often still too slow with regard
to testing. Why? Because
just as code doesn&amp;rsquo;t work until it has been {unit, integration}-tested, a
product doesn&amp;rsquo;t work until it has been user-tested. Again, if you are thinking
as a developer, you will be tempted to wait until you have an advanced prototype.
But, if you are thinking as a developer, you also know that you don&amp;rsquo;t wait until
the project is nearly complete to start adding {unit, integration} tests. This
is doubly true if your project involves hardware – just building the hardware
in the correct form factor can take months that you typically do not have.&lt;/p&gt;

&lt;p&gt;So put on your Survivor Hat and find ways to run user tests
on a prototype that doesn&amp;rsquo;t exist yet.&lt;/p&gt;

&lt;p&gt;UX can be tested on mockups. If it&amp;rsquo;s a GUI, a text interface or a braille
interface, draw it using any mockup tool, or
even on paper, grab some people in the hallway, see if they can guess how to
go from &amp;ldquo;I want to do this&amp;rdquo; to &amp;ldquo;I&amp;rsquo;m clicking the right button&amp;rdquo;. If it&amp;rsquo;s a
sound/voice UI, hide your mockup, grab some more people in the hallway, and
roleplay the speech recognition/text-to-speech engine, etc. Even wearable IoT
devices can be mocked up using wire, velcro, a smartphone and some bluetooth
accessories.&lt;/p&gt;

&lt;p&gt;Similarly, you can user-test algorithms. Add a mockup UX on top of it – again,
it can be as simple as paper – grab some people in the hallway and run the
algorithm. If the algorithm is simple enough, you can again roleplay it. If it is
more complex, turn it into a command-line/REPL toplevel and run it behind the
scenes.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s go further. Are you building an IoT device? If you are lucky, you can
quickly rig together a prototype running your code on top of a smartphone,
smart tv, alexa or any existing device that your future users already own –
and if no smart device has all the capabilities you need for testing your code,
maybe a Bluetooth thingy can provide the missing features. Or perhaps you can
prototype your software application as a browser add-on. Or detach a single
feature and turn it into an App.
Suddenly, instead of having access to a handful of potential alpha-testers,
you can put your code in front of potential millions.&lt;/p&gt;

&lt;p&gt;Again, regardless of the technique you use, spending a few weeks on mockups/early
prototypes will let you perform user test in parallel with the unit and integration
tests that you already run continuously. Much as unit/integration tests, user
tests will let you fix issues before they kill you. Hey, sometimes, &lt;a href=&#34;https://vr.google.com/cardboard/&#34;&gt;a prototype&lt;/a&gt;
can even &lt;em&gt;become&lt;/em&gt; &lt;a href=&#34;https://vr.google.com/daydream/&#34;&gt;a product&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;now-what&#34;&gt;Now what?&lt;/h1&gt;

&lt;p&gt;Well, hopefully, this blog post gave you a few ideas. Now, put on your
Survivor Hat and start thinking about ways to make your project survive until
it has a chance to have an impact!&lt;/p&gt;

&lt;p&gt;As a side-note, while I came up with the &amp;ldquo;Survivor Hat&amp;rdquo; as a narrative to
explain some of the techniques above to fellow developers and while we came up
with some of the ideas above mostly independently during Project Lighthouse,
this blog entry is largely about some of the &amp;ldquo;Lean Startup&amp;rdquo; concepts. So, if you
are interested you should find it easy to read more on the general idea.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Extracting text from images is not easy (who would have guessed?)</title>
      <link>https://yoric.github.io/post/extracting-text-from-images/</link>
      <pubDate>Sun, 01 Jan 2017 23:44:19 +0100</pubDate>
      
      <guid>https://yoric.github.io/post/extracting-text-from-images/</guid>
      <description>

&lt;p&gt;At its core, Lighthouse is an idea we have been discussing in Connected Devices: can we build a device that will help people with partial or total vision
disabilities?&lt;/p&gt;

&lt;p&gt;From there, we started a number of experiments. I figured out it was time to
braindump some of them.&lt;/p&gt;

&lt;h2 id=&#34;our-problem&#34;&gt;Our problem&lt;/h2&gt;

&lt;p&gt;Consider the following example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/10190/18631303/80cce95a-7e71-11e6-8a4e-bed542e6e7bb.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;How do we get from this beautiful picture of Mozilla&amp;rsquo;s Paris office to the text &amp;ldquo;PRIDE and PREJUDICE&amp;rdquo;, &amp;ldquo;Jane Austen&amp;rdquo;, &amp;ldquo;Great Books&amp;rdquo;, &amp;ldquo;Great Prices&amp;rdquo;, &amp;ldquo;$9.99&amp;rdquo;, &amp;ldquo;Super livres&amp;rdquo;, &amp;ldquo;Super prix&amp;rdquo;? That&amp;rsquo;s Canadian dollars, if you wonder.&lt;/p&gt;

&lt;p&gt;Too easy? What about this one?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/10190/18631577/13481e2a-7e73-11e6-9057-72aa048d7040.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Or that one?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/10190/18631784/45764bd2-7e74-11e6-9b2c-5ef823fd129a.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Not very easy to read, right? Now, remember one thing: these pictures were taken by people who can use a camera correctly because, well, they can see the image. If we want to build a device that will be useful for blind or vision-impaired users, we need a tool that can perform text recognition on images that are taken in much worse conditions or light or angle – or at least, we must be able to tell the user what&amp;rsquo;s the problem with the image.&lt;/p&gt;

&lt;p&gt;Actually, extracting text from an image is a Hard Problem &amp;trade;. Sufficiently hard that there are tracks in research conferences that deal with this kind of problem. But, surely, in this age of Deep Learning, there must be a Magic Cloud API &amp;trade; that can do that, right?&lt;/p&gt;

&lt;h2 id=&#34;looking-for-a-cloud-api&#34;&gt;Looking for a Cloud API&lt;/h2&gt;

&lt;p&gt;So, we set out to benchmarkg existing APIs that claim that they can perform text recognition on images. This includes Google&amp;rsquo;s APIs, IBM/AlchemyVision&amp;rsquo;s APIs, CloudSight&amp;rsquo;s APIs, Microsoft&amp;rsquo;s Vision APIs.&lt;/p&gt;

&lt;p&gt;If you are curious, I invite you to try with the three samples above. The results are limited. In some cases, we get acceptable recognition. In most cases, the APIs just can&amp;rsquo;t help.&lt;/p&gt;

&lt;p&gt;To make things worse, these Magic Cloud APIs are, of course, not free. Using them comes at a cost, in terms of device bandwith, data plan, and actual per-request dollars.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Bottom line&lt;/em&gt; These Magic Cloud APIs are acceptable for a demo, but are far from sufficient if we want to build a tool useful for an actual blind person.&lt;/p&gt;

&lt;h2 id=&#34;looking-for-an-offline-api&#34;&gt;Looking for an offline API&lt;/h2&gt;

&lt;p&gt;These days, the recommended OCR library in the open-source world seems to be Tesseract. Testing it against any of the images above yields strictly nothing usable.&lt;/p&gt;

&lt;p&gt;This is not a surprise: Tesseract is fine-tuned for use in a scanner. Give it black letters, white paper, no textures, and a simple layout, and Tesseract will return text with an acceptable quality. Give it anything else and Tesseract will assume that it&amp;rsquo;s black letters, white paper, no textures and a simple layout, and will attempt to interpret every single line, every single intersection as a letter. And fail.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Bottom line&lt;/em&gt; On its own, Tesseract is not usable.&lt;/p&gt;

&lt;h2 id=&#34;looking-for-a-way-out&#34;&gt;Looking for a way out&lt;/h2&gt;

&lt;p&gt;To summarize, I believe that we have the following problems:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Magic Cloud APIs attempt to solve all the issues in the world at once and are consequently great at nothing;&lt;/li&gt;
&lt;li&gt;Tesseract is too specific to a single use case which is not ours, and consequently not usable as-is for our project.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;working-around-the-limitations-of-tesseract&#34;&gt;Working around the limitations of Tesseract&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s start with problem 2., because there are many options to solve it. If Tesseract cannot be used on unclean text, we &lt;em&gt;just&lt;/em&gt; need to clean up the text, using well-known or not-so-well-known Computer Vision algorithms, before feeding it to Tesseract. A quick look at Stack Overflow indicates that we are hardly the first project attempting to do this and that there are a number of solutions that &lt;em&gt;should&lt;/em&gt; work. Sadly, not much feedback on whether they &lt;em&gt;actually&lt;/em&gt; work, but hey, that&amp;rsquo;s what experimenting is all about, right?&lt;/p&gt;

&lt;p&gt;If we succeed, we will have at hand a super-Tesseract, well-adapted to &lt;em&gt;some categories of images&lt;/em&gt;. A super-Tesseract that we can further train to adapt it to the images in which we are interested. Say, text recognition on packaging, with the lighting conditions available in a supermarket.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We have a plan&lt;/em&gt; we don&amp;rsquo;t need to build a full OCR engine, we just need a flexible text cleanup/extraction tool.&lt;/p&gt;

&lt;h3 id=&#34;working-around-poor-lights-angle&#34;&gt;Working around poor lights, angle, &amp;hellip;&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s be pessimistic and assume that, despite our best efforts, our super-Tesseract will not be able to handle poor light, poor angle, etc. any better than the Magic Cloud APIs.&lt;/p&gt;

&lt;p&gt;Super-Tesseract will still have a few aces up its metaphoric sleeve:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If we run it locally, on the user&amp;rsquo;s device, it is not limited by bandwidth, data plan or per-request cost;&lt;/li&gt;
&lt;li&gt;While performing actual text recognition is relatively slow on a smartphone-style device, we have hopes that performing text detection and cleanup can be made quite fast.&lt;/li&gt;
&lt;li&gt;As part of super-Tesseract, we can build algorithms that can actually determine whether the angle is wrong, or the lightning is bad, etc. All sorts of information that the Magic Cloud APIs do not give us.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Put together, these two aces open several possibilities:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We can afford to perform at least the first few steps of super-Tesseract &lt;em&gt;on videos&lt;/em&gt;, rather than on a single image, which will give us more samples from which to extract clean text.&lt;/li&gt;
&lt;li&gt;We can afford to perform these same steps several times, using several different filters and parameters, to offset the light, angle, etc.&lt;/li&gt;
&lt;li&gt;We can provide feedback for the users, actually telling them what&amp;rsquo;s wrong, if they need to turn the cereal bar, or move the camera closer, or if the text is just simply unreadable because of too many textures.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Bottom line&lt;/em&gt;: We need a very fast implementation of text clean-up/extraction that can also diagnose its own problems.&lt;/p&gt;

&lt;h2 id=&#34;towards-super-tesseract&#34;&gt;Towards super-Tesseract&lt;/h2&gt;

&lt;p&gt;At this stage, my hopes are to perform the following steps:&lt;/p&gt;

&lt;p&gt;a. locating the text in an image;
b. removing everything that isn&amp;rsquo;t the text;
c. turning it if necessary to make sure that it&amp;rsquo;s horizontal;
d. now that the text is as clean as in a book, feed it to Tesseract.&lt;/p&gt;

&lt;p&gt;During the past few weeks, I have been experimenting with algorithms for these things. All the problems are tricky, but I am making progress.&lt;/p&gt;

&lt;p&gt;a. An algorithm based on &lt;a href=&#34;https://www.microsoft.com/en-us/research/publication/stroke-width-transform/[Stroke&#34;&gt;https://www.microsoft.com/en-us/research/publication/stroke-width-transform/[Stroke&lt;/a&gt; Width Transforms] (SWT) has very good results on locating the text in an image. I have tested the implementation of this algorithm in the &lt;a href=&#34;https://github.com/liuliu/ccv[CCV&#34;&gt;https://github.com/liuliu/ccv[CCV&lt;/a&gt;] library and it has very good performance, both in terms of speed and in terms of results. So far, so good. As I am prototyping in Rust, I have published &lt;a href=&#34;https://github.com/Yoric/ccv.rs[very&#34;&gt;https://github.com/Yoric/ccv.rs[very&lt;/a&gt; early bindings] – feel free to use them if you want to toy with this algorithm.&lt;/p&gt;

&lt;p&gt;b. I have tested several algorithms for cleaning up everything that isn&amp;rsquo;t text. Unfortunately, so far, these algorithms perform quite poorly, both in terms of speed and accuracy. Looking harder at SWT, I believe that this algorithm can be extended to perform the cleanup. I am currently working such an extension, but it might be a few weeks before I can seriously test it.&lt;/p&gt;

&lt;p&gt;c. An algorithm called Deskewing is rumored to have good results. I have located an implementation of this as part of a library called
&lt;a href=&#34;http://www.leptonica.org/[Leptonica&#34;&gt;http://www.leptonica.org/[Leptonica&lt;/a&gt;]. The implementation is very fast, but it doesn&amp;rsquo;t work too well on my samples. I need to investigate this further. It may be a problem with my samples, which are not sufficiently cleaned up at this stage, or with the algorithm, or even with me misunderstanding the results of the algorithm. Again, I have published &lt;a href=&#34;https://github.com/Yoric/lepton.rs[minimal&#34;&gt;https://github.com/Yoric/lepton.rs[minimal&lt;/a&gt; Rust bindings for Leptonica], to simplify experimentation.&lt;/p&gt;

&lt;h2 id=&#34;things-not-text&#34;&gt;Things not text&lt;/h2&gt;

&lt;p&gt;Byproduct of the study: there are several very important pieces of
information on boxes and clothes that would be useful for blind or vision-impaired users but that most likely cannot be treated by OCR.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m thinking of logos (e.g. no amount of OCR will recognize
Coca Cola, Kellogs, &amp;hellip;) and other symbols (e.g. non-recycle plastic,
toxicity warning, hand wash only, &amp;hellip;)&lt;/p&gt;

&lt;p&gt;At some point, we will certainly want to recognize them. I suspect that neural networks/deep learning would be pretty good for that purpose, but I have no idea how much effort that would take. I also suspect that Magic Cloud APIs already handle these quite nicely already, so at least for a prototype, it might be useful to use these as a baseline.&lt;/p&gt;

&lt;h1 id=&#34;commenting&#34;&gt;Commenting&lt;/h1&gt;

&lt;p&gt;You can leave comments &lt;a href=&#34;https://github.com/Yoric/yoric.github.io/issues/1[on&#34;&gt;https://github.com/Yoric/yoric.github.io/issues/1[on&lt;/a&gt; github] or using Disqus below. I haven&amp;rsquo;t decided what&amp;rsquo;s best yet.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>