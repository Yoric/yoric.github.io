<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Iot on Il y a du thé renversé au bord de la table !</title>
    <link>https://yoric.github.io/categories/iot/index.xml</link>
    <description>Recent content in Iot on Il y a du thé renversé au bord de la table !</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <atom:link href="https://yoric.github.io/categories/iot/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Project Lighthouse: A post-mortem</title>
      <link>https://yoric.github.io/post/post-mortem-lighthouse/</link>
      <pubDate>Mon, 20 Feb 2017 10:44:33 +0100</pubDate>
      
      <guid>https://yoric.github.io/post/post-mortem-lighthouse/</guid>
      <description>

&lt;p&gt;A few weeks ago, Mozilla pulled the plug on its Connected Devices Experiment:
a bunch of internal non-profit hardware-related startups. One of our main
objectives was to determine if we could come up with designs that could help
turn the tide against the spyware-riddled and gruyère-level security devices that are
currently being offered (or pushed) to unwary users.&lt;/p&gt;

&lt;p&gt;One of the startups was &lt;a href=&#34;https://wiki.mozilla.org/Connected_Devices/Projects/Project_Lighthouse&#34;&gt;&lt;strong&gt;Project Lighthouse&lt;/strong&gt;&lt;/a&gt;. We tried to provide an
affordable, simple and privacy-friendly tool for people suffering from
vision impairment and who needed help in their daily life. While Mozilla
closed the experiment before we could release anything useful, we still came
up with a few nifty ideas. Here is a post-mortem, in the hope that, maybe,
someone will be able to succeed where we couldn&amp;rsquo;t.&lt;/p&gt;

&lt;h1 id=&#34;the-problems-at-hand&#34;&gt;The problems at hand&lt;/h1&gt;

&lt;p&gt;If you suffer from sever enough vision impairment, there are many things that
you can do on your own, but also many things for which you need help. Things
such as interacting with printed text, with identical-to-the-touch bottles or
cereal boxes, with the color of your clothes, etc.&lt;/p&gt;

&lt;p&gt;The World Health Organization estimates that there are ~285 millions of
vision-impaired people in the world, including ~30 millions who are entirely
blind. Roughly 85% of them live below poverty level, which means that they
cannot afford a helper (unless it&amp;rsquo;s a family member), nor an expensive gadget.&lt;/p&gt;

&lt;p&gt;So, how can we help them?&lt;/p&gt;

&lt;h1 id=&#34;real-life-text-recognition&#34;&gt;Real-life text recognition&lt;/h1&gt;

&lt;p&gt;At first, we considered helping vision-impaired people interact with printed text.
The general idea was to develop a camera that could read out the text from an
image, and then iterate on accessibility and form factor.&lt;/p&gt;

&lt;p&gt;I have written in details about the difficulties of the operation in
&lt;a href=&#34;../extracting-text-from-images&#34;&gt;another blog entry&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Among the questions at hand:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;is it feasible?&lt;/li&gt;
&lt;li&gt;would such a device need an Internet connection?&lt;/li&gt;
&lt;li&gt;how much CPU/memory would the device need?&lt;/li&gt;
&lt;li&gt;what accuracy could we expect?&lt;/li&gt;
&lt;li&gt;would it be limited to English?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;We concluded that we could finish implementing a reasonable text extraction
with a few more weeks of work. This extraction would work just as well if the
object was held upside-down or in diagonal, and should, in theory, work with
all the common fonts used for indo-european languages. We suspected that the
algorithms would need additional work for semitic or asian languages.&lt;/p&gt;

&lt;p&gt;We also realized that the underlying algorithms relied upon a number of magic
constants (e.g. contrast) and that we would need additional work to tune these
constants to actual use cases. We could not rely upon stock pictures, for
instance, as we could expect that vision-impaired users would not be sensitive
the lighting conditions, or shadows or the framing in the same manner as
professional or even casual photographers. It was not clear how we could capture
a sample sufficiently large to extract relevant constants.&lt;/p&gt;

&lt;p&gt;We benchmarked that text extraction needed lots of CPU, so would probably rely
either upon a cloud service (bad for privacy, expensive for many users) or upon
a beefy cellphone (expensive for many users).&lt;/p&gt;

&lt;p&gt;Further, we determined that extraction would be unfeasible when the orientation
of a single line of text changed too much, e.g. on round pill bottles, or on
bags of potato chips.&lt;/p&gt;

&lt;p&gt;Finally, it was clear that extracting texts and recognizing logos were two very
different issues. No amount of text extraction would be able to parse the
calligraphy of the Coca-Cola logo, or to understand that a &amp;ldquo;G&amp;rdquo; with a specific
color combination could mean &amp;ldquo;Google&amp;rdquo;, or that &amp;ldquo;moz://a&amp;rdquo; could mean &amp;ldquo;Mozilla&amp;rdquo;.&lt;/p&gt;

&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;We decided that text extraction should not be the main feature of the device,
but rather something that we would add at a later stage, probably after logo
recognition.&lt;/p&gt;

&lt;h1 id=&#34;object-tagging&#34;&gt;Object tagging&lt;/h1&gt;

&lt;p&gt;A second avenue was helping vision-impaired users interact with objects that
are difficult to differentiate by touch.&lt;/p&gt;

&lt;p&gt;We envisioned three possible ways to proceed in this direction:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;let the user take pictures of the objects, recognize what the object was;&lt;/li&gt;
&lt;li&gt;let the user take pictures of the objects, record a description (
&amp;ldquo;noodles, they need to cook 10 minutes&amp;rdquo;,
&amp;ldquo;hat, goes well with my bowtie&amp;rdquo;, &amp;hellip;), use computer vision algorithms
to later recognize objects from the internal database;&lt;/li&gt;
&lt;li&gt;let the user take pictures of the objects, record a description, use
deep learning to later recognize objects from the internal database.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We decided to avoid 1. for a minimal viable product, as it would require either a large
database and a cloud service, or reliance upon third-party cloud services,
which in turn meant:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;dependency on an always-present internet connection;&lt;/li&gt;
&lt;li&gt;many potential privacy issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We also decided to avoid 3. for a minimal viable product, as we did not have
the in-team skills to proceed with deep learning. We meant to return to 3. in
a relatively close future, though.&lt;/p&gt;

&lt;p&gt;So, we went ahead with option 2. Among the questions at hand:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;is it feasible?&lt;/li&gt;
&lt;li&gt;would such a device need an Internet connection?&lt;/li&gt;
&lt;li&gt;how much CPU/memory would the device need?&lt;/li&gt;
&lt;li&gt;what accuracy could we expect?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;results-1&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;After a few weeks of coding, we managed to produce a pretty nice prototype,
using a Raspberry Pi, OpenCV and Python. In this limited span of time, we did
not seriously attempt to resolve issues such as shaky cameras, but otherwise,
the prototype worked nicely. Recording or finding an object in the database was
nearly instantaneous (for a database of a dozen recorded objects only), without
an internet connection, and accuracy was good (we did not attempt to quantify it).&lt;/p&gt;

&lt;p&gt;We needed to come up with a method to let the camera differentiate the object
from its background. In the absence of sophisticated stereoscopic cameras, we
used two tricks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ask the user to show the object, then hide it; or&lt;/li&gt;
&lt;li&gt;ask the user to shake the object in from of the camera.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We tested some of the prototypes with real users, got good feedback
and figured that we could iterate from there. Our prototypes cost ~$50 to build,
a far cry from the $1,500+ we found for other devices for vision-impaired users.&lt;/p&gt;

&lt;p&gt;Also, other tests with a few fully blind users confirmed that aiming with a
camera was not a problem. There were difficulties with judging distance or
light, of course, but we expected that these could be mitigated by judicious
algorithms.&lt;/p&gt;

&lt;p&gt;So, we decided to proceed from this prototype.&lt;/p&gt;

&lt;h1 id=&#34;form-factor&#34;&gt;Form factor&lt;/h1&gt;

&lt;p&gt;Once we were reasonably convinced that we could come up with a technology to
make the device work, we started working on the form factor. We came up with
several ideas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a camera in a necklace;&lt;/li&gt;
&lt;li&gt;a camera attached to the user&amp;rsquo;s glasses;&lt;/li&gt;
&lt;li&gt;a camera that could attach to many places, including a supermarket trolley.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unfortunately, the project was canceled before we could make much headway in
the form factor.&lt;/p&gt;

&lt;p&gt;An important lesson, though, is that going from design to industrial prototype
takes lots of time. I do not have hard numbers, as we didn&amp;rsquo;t reach that stage,
but my personal estimate puts this around &lt;strong&gt;6 months&lt;/strong&gt;. Which means that we
needed a way to test our code and our form factor prototypes with real users.&lt;/p&gt;

&lt;p&gt;We did not get the opportunity to test the form factor, although &lt;a href=&#34;how-to-survive.html&#34;&gt;I discussed in
another post&lt;/a&gt; some of the techniques that we could have
used for this purpose.&lt;/p&gt;

&lt;h1 id=&#34;app&#34;&gt;App&lt;/h1&gt;

&lt;p&gt;It would have been unrealistic to wait for the industrial prototypes before
proceeding, so we came up with an idea to test our code in parallel with the
external and industrial design.&lt;/p&gt;

&lt;p&gt;The idea was to port our algorithms to a device that a sufficiently large number
of our user base already owned. While developing for Android would have been
nice, our statistics (US only) showed almost all vision-impaired users who owned
a smartphone were using iPhones, so we did not have much of a choice. We decided
to develop an application for iPhone, release it first on Apple&amp;rsquo;s TestFlight
(for alpha-testers), then the App Store.&lt;/p&gt;

&lt;p&gt;Additionally, we expected that some users who already owned an iPhone could be
interested in the application, even without an optimal form factor. For other users, we still intended to provide an
inexpensive full-featured device.&lt;/p&gt;

&lt;h2 id=&#34;results-2&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;We managed to port our code relatively easily from Linux to iOS but the project
was canceled while we were still 1-2 weeks away from a first pre-alpha release,
an estimated 4-6 weeks before we could go beta.&lt;/p&gt;

&lt;p&gt;Performance was excellent, which is no surprise as iPhones are largely more
powerful than Raspberry Pis.&lt;/p&gt;

&lt;p&gt;While population numbers make it clear that iOS was the right platform for early
testing and would be the only reasonable target for a smartphone-only application
for vision-impaired people &lt;em&gt;in the US&lt;/em&gt;, we had a terrible experience developing
for iPhone.&lt;/p&gt;

&lt;p&gt;Since we needed to interact with a C++ library for computer vision, we needed
to code in three languages: C++ (for the computer vision), Swift (for the UX)
and Obj-C++ (the unnatural grandchild of C), with two different threading models
and three different memory management paradigms (C, C++, Swift/Obj-C). Getting
to a stage at which we could perform callbacks from C++ to Swift took some
serious digging and understanding Obj-C++&amp;rsquo;s very &lt;em&gt;interesting&lt;/em&gt; memory model.&lt;/p&gt;

&lt;p&gt;The signature mechanism seems to be designed largely to get developers to pay
(either $99 per developer or $199 per team), which is quite adverse to both
open-source contributions and to continuous integration. It also makes it
hard/impossible to distribute binaries for testing, even to your non-developer
teammates.&lt;/p&gt;

&lt;p&gt;Also, the command-line tools offer now reliable way to launch unit/integration
tests, plus they have breaking syntax changes – these two points contribute to
making continuous integration needlessly difficult.&lt;/p&gt;

&lt;h1 id=&#34;not-tested-crowdsourcing-help&#34;&gt;Not tested: Crowdsourcing help&lt;/h1&gt;

&lt;p&gt;One of the features we were considering was a way to crowdsource requests for
assistance. Consider a vision-impaired user with a question that cannot be
easily answered by a computer: &amp;ldquo;My arm hurts, is there anything strange on it?&amp;rdquo;,
&amp;ldquo;Does this dress match this hat?&amp;rdquo;, etc. How can we put the user in touch with
someone willing to help?&lt;/p&gt;

&lt;p&gt;Broadly speaking, our idea was the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;extend the camera device and application with the ability to record a question;&lt;/li&gt;
&lt;li&gt;develop a web service to gather the questions, their answers, moderation, meta-moderation, &amp;hellip;;&lt;/li&gt;
&lt;li&gt;develop a web application front-end to the web service;&lt;/li&gt;
&lt;li&gt;wrap this web application as smartphone apps and browser add-ons for all major
browsers, designed so that a user could spend ~5-20 seconds helping whenever they
felt like it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This left many issues unanswered, such as how to avoid spamming users with
inappropriate content, how to handle privacy (&amp;ldquo;I have photographed my credit
card by accident&amp;rdquo;), etc.&lt;/p&gt;

&lt;p&gt;We only had time for the first paper explorations of the topic, but we
believed that such an open-source crowdsourcing platform for requesting
5-20 seconds help could have become very useful for many projects.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Things to do before you[r project] die[s]</title>
      <link>https://yoric.github.io/post/how-to-survive/</link>
      <pubDate>Fri, 17 Feb 2017 00:10:34 +0100</pubDate>
      
      <guid>https://yoric.github.io/post/how-to-survive/</guid>
      <description>&lt;p&gt;It&amp;rsquo;s no secret that all projects die, eventually, and that most projects die
before they have had a chance to have any impact. This is true of personal projects,
of open-source projects, of startup projects, of hardware projects, and more.&lt;/p&gt;

&lt;p&gt;A few days ago, &lt;a href=&#34;https://wiki.mozilla.org/Connected_Devices/Projects/Project_Lighthouse&#34;&gt;the IoT project on which I was working&lt;/a&gt;
did just that. It was funded, it was open-source, it was open-hardware, and it
still died before having the opportunity to release anything useful. In our case,
we got pretty far, didn&amp;rsquo;t run out of funds, and were a little polish away from
putting our first alpha-testers in front of a pretty advanced prototype before
a higher-level strategy pivot killed a number of projects at once.&lt;/p&gt;

&lt;p&gt;Ah, well, risks of the trade.&lt;/p&gt;

&lt;p&gt;Still, working on it, we managed to learn or come up with a few tricks
to increase the chances of surviving at least long enough to release a product.
I&amp;rsquo;ll try to summarize and explain some of these tricks here. Hopefully, if you
have a project, regardless of funding and medium, you may find some of this
summary useful.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;death-by-nobody-will-use-your-project&#34;&gt;Death by Nobody-will-use-your-project&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;tl;dr&lt;/p&gt;

&lt;p&gt;There are many good ways to test an idea way before the prototype is solid
enough to show it up. Use them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You have an idea. It&amp;rsquo;s certainly a great idea. Your friends and your family
probably love it. A few geeks around you are enthusiastic, it can change the world!&lt;/p&gt;

&lt;p&gt;At this stage, you are eager to start coding/designing/burning circuits/&amp;hellip;
And that&amp;rsquo;s great. By all means, if you&amp;rsquo;re enjoying yourself, please carry on.
However, before you start committing to the project anything you might regret –
leaving your day job, spending your money or your investors&amp;rsquo; – you should make
sure that your project has a chance to gain users and traction.&lt;/p&gt;

&lt;p&gt;If you come from a technological background, you may believe that you need to
put your project in front of users to get feedback, which means that you need
a pretty advanced prototype. &lt;em&gt;If you do this, you considerably increase your
chances of your project dying on you.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So time to put your Survivor Hat.&lt;/p&gt;

&lt;p&gt;There are many ways to get user feedback while the project is in its
infancy. Doing this used to be long, expensive and – just as important for an indie
project – boring. These days, there are a few services that do it for you
and actually bring money to your project in case of success. Yes, I am talking of &lt;a href=&#34;http://kickstarter.com/&#34;&gt;Kickstarter&lt;/a&gt;, &lt;a href=&#34;http://indiegogo.com/&#34;&gt;Indiegogo&lt;/a&gt;
&amp;amp; &lt;a href=&#34;https://en.wikipedia.org/wiki/Comparison_of_crowdfunding_services&#34;&gt;co&lt;/a&gt;. If you
believed that these crowdfunding platforms were about gathering money,
welcome to the worst-kept secret of the 2010s:
they are actually marketing platforms that will let you find out &lt;em&gt;if people are
willing to spend money on your project&lt;/em&gt;. Which, in many cases, is a pretty good
measure of whether you should greenlight your own project. Just be sure to be
honest regarding the current state of your project and how the money will be
used. If you lie, even with the best of intentions, your funders will eventually
find out and they will be legitimately angry.&lt;/p&gt;

&lt;p&gt;An alternative, depending on what you&amp;rsquo;re doing, is to open a website and start
taking pre-orders (morally dubious), start letting interested parties subscribe
to an announcement mailing-list (better), or get creative. I have even seen weirder variants in
which non-technical founders started a fairly sophisticated online service
before they even hired a single developer – by hiring underpaid students to actually do
the work of the machine, at least long enough to determine whether users would
actually use them (neat trick, morally&amp;hellip; weird). I have also seen at least one startup-turned-internet-giant who implemented only a subset of the promised
features and measured how many users clicked on buttons that were not
implemented yet (not a trick I like, but it worked for them).&lt;/p&gt;

&lt;p&gt;Note that all these techniques help you determine your potential end users.
Unless your end users are developers, that&amp;rsquo;s entirely orthogonal to getting
people to take part in the development of your project.&lt;/p&gt;

&lt;p&gt;This list is clearly not exhaustive. Now that you have your Survivor Hat on,
you should be able to come up alternatives if you need to.&lt;/p&gt;

&lt;p&gt;Regardless of the technique you use, spending a few days of work to measuring
your potential audience can save you from disastrous choices for your career
(or savings, or startup, &amp;hellip;). As a bonus, putting together some communication
will also serve to put your own thoughts in order, decide of your priorities,
get feedback from your future users. Also, if you need money, it can serve to
fund you (crowdfunding) and/or to convince investors.&lt;/p&gt;

&lt;h1 id=&#34;death-by-tests-need-users-need-tests&#34;&gt;Death by Tests-need-users-need-tests&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;tl;dr&lt;/p&gt;

&lt;p&gt;There are many good ways to test algorithms, UX, accessibility, &amp;hellip; before the
prototype is solid enough to show it up. Use them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At this stage, I&amp;rsquo;m assuming that you have identified your user population and
the conditions in which your project will be used. You will, of course, need
to adapt – you will need different testers for an OCR-for-the-blind or for a
superbowl-adblocker. For narrative purposes, though, I&amp;rsquo;ll pretend that your testers
somehow lurk in your hallway.&lt;/p&gt;

&lt;p&gt;So, your users are interested, your project is going full speed ahead. Great.
But, like it or not, full speed ahead is often still too slow with regard
to testing. Why? Because
just as code doesn&amp;rsquo;t work until it has been {unit, integration}-tested, a
product doesn&amp;rsquo;t work until it has been user-tested. Again, if you are thinking
as a developer, you will be tempted to wait until you have an advanced prototype.
But, if you are thinking as a developer, you also know that you don&amp;rsquo;t wait until
the project is nearly complete to start adding {unit, integration} tests. This
is doubly true if your project involves hardware – just building the hardware
in the correct form factor can take months that you typically do not have.&lt;/p&gt;

&lt;p&gt;So put on your Survivor Hat and find ways to run user tests
on a prototype that doesn&amp;rsquo;t exist yet.&lt;/p&gt;

&lt;p&gt;UX can be tested on mockups. If it&amp;rsquo;s a GUI, a text interface or a braille
interface, draw it using any mockup tool, or
even on paper, grab some people in the hallway, see if they can guess how to
go from &amp;ldquo;I want to do this&amp;rdquo; to &amp;ldquo;I&amp;rsquo;m clicking the right button&amp;rdquo;. If it&amp;rsquo;s a
sound/voice UI, hide your mockup, grab some more people in the hallway, and
roleplay the speech recognition/text-to-speech engine, etc. Even wearable IoT
devices can be mocked up using wire, velcro, a smartphone and some bluetooth
accessories.&lt;/p&gt;

&lt;p&gt;Similarly, you can user-test algorithms. Add a mockup UX on top of it – again,
it can be as simple as paper – grab some people in the hallway and run the
algorithm. If the algorithm is simple enough, you can again roleplay it. If it is
more complex, turn it into a command-line/REPL toplevel and run it behind the
scenes.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s go further. Are you building an IoT device? If you are lucky, you can
quickly rig together a prototype running your code on top of a smartphone,
smart tv, alexa or any existing device that your future users already own –
and if no smart device has all the capabilities you need for testing your code,
maybe a Bluetooth thingy can provide the missing features. Or perhaps you can
prototype your software application as a browser add-on. Or detach a single
feature and turn it into an App.
Suddenly, instead of having access to a handful of potential alpha-testers,
you can put your code in front of potential millions.&lt;/p&gt;

&lt;p&gt;Again, regardless of the technique you use, spending a few weeks on mockups/early
prototypes will let you perform user test in parallel with the unit and integration
tests that you already run continuously. Much as unit/integration tests, user
tests will let you fix issues before they kill you. Hey, sometimes, &lt;a href=&#34;https://vr.google.com/cardboard/&#34;&gt;a prototype&lt;/a&gt;
can even &lt;em&gt;become&lt;/em&gt; &lt;a href=&#34;https://vr.google.com/daydream/&#34;&gt;a product&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;now-what&#34;&gt;Now what?&lt;/h1&gt;

&lt;p&gt;Well, hopefully, this blog post gave you a few ideas. Now, put on your
Survivor Hat and start thinking about ways to make your project survive until
it has a chance to have an impact!&lt;/p&gt;

&lt;p&gt;As a side-note, while I came up with the &amp;ldquo;Survivor Hat&amp;rdquo; as a narrative to
explain some of the techniques above to fellow developers and while we came up
with some of the ideas above mostly independently during Project Lighthouse,
this blog entry is largely about some of the &amp;ldquo;Lean Startup&amp;rdquo; concepts. So, if you
are interested you should find it easy to read more on the general idea.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>