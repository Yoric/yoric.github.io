<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ocr on Il y a du thé renversé au bord de la table !</title>
    <link>https://yoric.github.io/tags/ocr/</link>
    <description>Recent content in Ocr on Il y a du thé renversé au bord de la table !</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>© This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License，please give source if you likes to quote or reproduce.</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 23:44:19 +0100</lastBuildDate>
    
	<atom:link href="https://yoric.github.io/tags/ocr/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Extracting text from images is not easy (who would have guessed?)</title>
      <link>https://yoric.github.io/post/extracting-text-from-images/</link>
      <pubDate>Sun, 01 Jan 2017 23:44:19 +0100</pubDate>
      
      <guid>https://yoric.github.io/post/extracting-text-from-images/</guid>
      <description>At its core, Lighthouse is an idea we have been discussing in Connected Devices: can we build a device that will help people with partial or total vision disabilities?
From there, we started a number of experiments. I figured out it was time to braindump some of them.
Our problem Consider the following example:
How do we get from this beautiful picture of Mozilla&amp;rsquo;s Paris office to the text &amp;ldquo;PRIDE and PREJUDICE&amp;rdquo;, &amp;ldquo;Jane Austen&amp;rdquo;, &amp;ldquo;Great Books&amp;rdquo;, &amp;ldquo;Great Prices&amp;rdquo;, &amp;ldquo;$9.</description>
    </item>
    
  </channel>
</rss>