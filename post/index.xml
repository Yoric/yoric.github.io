<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Il y a du thé renversé au bord de la table !</title>
    <link>https://yoric.github.io/post/index.xml</link>
    <description>Recent content in Posts on Il y a du thé renversé au bord de la table !</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Feb 2017 10:44:33 +0100</lastBuildDate>
    <atom:link href="https://yoric.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Project Lighthouse: A post-mortem</title>
      <link>https://yoric.github.io/post/post-mortem-lighthouse/</link>
      <pubDate>Mon, 20 Feb 2017 10:44:33 +0100</pubDate>
      
      <guid>https://yoric.github.io/post/post-mortem-lighthouse/</guid>
      <description>

&lt;p&gt;A few weeks ago, Mozilla pulled the plug on its Connected Devices Experiment:
a bunch of internal non-profit hardware-related startups. One of our main
objectives was to determine if we could come up with designs that could help
turn the tide against the spyware-riddled and gruyère-level security devices that are
currently being offered (or pushed) to unwary users.&lt;/p&gt;

&lt;p&gt;One of the startups was &lt;a href=&#34;https://wiki.mozilla.org/Connected_Devices/Projects/Project_Lighthouse&#34;&gt;&lt;strong&gt;Project Lighthouse&lt;/strong&gt;&lt;/a&gt;. We tried to provide an
affordable, simple and privacy-friendly tool for people suffering from
vision impairment and who needed help in their daily life. While Mozilla
closed the experiment before we could release anything useful, we still came
up with a few nifty ideas. Here is a post-mortem, in the hope that, maybe,
someone will be able to succeed where we couldn&amp;rsquo;t.&lt;/p&gt;

&lt;h1 id=&#34;the-problems-at-hand&#34;&gt;The problems at hand&lt;/h1&gt;

&lt;p&gt;If you suffer from sever enough vision impairment, there are many things that
you can do on your own, but also many things for which you need help. Things
such as interacting with printed text, with identical-to-the-touch bottles or
cereal boxes, with the color of your clothes, etc.&lt;/p&gt;

&lt;p&gt;The World Health Organization estimates that there are ~285 millions of
vision-impaired people in the world, including ~30 millions who are entirely
blind. Roughly 85% of them live below poverty level, which means that they
cannot afford a helper (unless it&amp;rsquo;s a family member), nor an expensive gadget.&lt;/p&gt;

&lt;p&gt;So, how can we help them?&lt;/p&gt;

&lt;h1 id=&#34;real-life-text-recognition&#34;&gt;Real-life text recognition&lt;/h1&gt;

&lt;p&gt;At first, we considered helping vision-impaired people interact with printed text.
The general idea was to develop a camera that could read out the text from an
image, and then iterate on accessibility and form factor.&lt;/p&gt;

&lt;p&gt;I have written in details about the difficulties of the operation in
&lt;a href=&#34;../extracting-text-from-images&#34;&gt;another blog entry&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Among the questions at hand:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;is it feasible?&lt;/li&gt;
&lt;li&gt;would such a device need an Internet connection?&lt;/li&gt;
&lt;li&gt;how much CPU/memory would the device need?&lt;/li&gt;
&lt;li&gt;what accuracy could we expect?&lt;/li&gt;
&lt;li&gt;would it be limited to English?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;We concluded that we could finish implementing a reasonable text extraction
with a few more weeks of work. This extraction would work just as well if the
object was held upside-down or in diagonal, and should, in theory, work with
all the common fonts used for indo-european languages. We suspected that the
algorithms would need additional work for semitic or asian languages.&lt;/p&gt;

&lt;p&gt;We also realized that the underlying algorithms relied upon a number of magic
constants (e.g. contrast) and that we would need additional work to tune these
constants to actual use cases. We could not rely upon stock pictures, for
instance, as we could expect that vision-impaired users would not be sensitive
the lighting conditions, or shadows or the framing in the same manner as
professional or even casual photographers. It was not clear how we could capture
a sample sufficiently large to extract relevant constants.&lt;/p&gt;

&lt;p&gt;We benchmarked that text extraction needed lots of CPU, so would probably rely
either upon a cloud service (bad for privacy, expensive for many users) or upon
a beefy cellphone (expensive for many users).&lt;/p&gt;

&lt;p&gt;Further, we determined that extraction would be unfeasible when the orientation
of a single line of text changed too much, e.g. on round pill bottles, or on
bags of potato chips.&lt;/p&gt;

&lt;p&gt;Finally, it was clear that extracting texts and recognizing logos were two very
different issues. No amount of text extraction would be able to parse the
calligraphy of the Coca-Cola logo, or to understand that a &amp;ldquo;G&amp;rdquo; with a specific
color combination could mean &amp;ldquo;Google&amp;rdquo;, or that &amp;ldquo;moz://a&amp;rdquo; could mean &amp;ldquo;Mozilla&amp;rdquo;.&lt;/p&gt;

&lt;h2 id=&#34;conclusions&#34;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;We decided that text extraction should not be the main feature of the device,
but rather something that we would add at a later stage, probably after logo
recognition.&lt;/p&gt;

&lt;h1 id=&#34;object-tagging&#34;&gt;Object tagging&lt;/h1&gt;

&lt;p&gt;A second avenue was helping vision-impaired users interact with objects that
are difficult to differentiate by touch.&lt;/p&gt;

&lt;p&gt;We envisioned three possible ways to proceed in this direction:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;let the user take pictures of the objects, recognize what the object was;&lt;/li&gt;
&lt;li&gt;let the user take pictures of the objects, record a description (
&amp;ldquo;noodles, they need to cook 10 minutes&amp;rdquo;,
&amp;ldquo;hat, goes well with my bowtie&amp;rdquo;, &amp;hellip;), use computer vision algorithms
to later recognize objects from the internal database;&lt;/li&gt;
&lt;li&gt;let the user take pictures of the objects, record a description, use
deep learning to later recognize objects from the internal database.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We decided to avoid 1. for a minimal viable product, as it would require either a large
database and a cloud service, or reliance upon third-party cloud services,
which in turn meant:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;dependency on an always-present internet connection;&lt;/li&gt;
&lt;li&gt;many potential privacy issues.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We also decided to avoid 3. for a minimal viable product, as we did not have
the in-team skills to proceed with deep learning. We meant to return to 3. in
a relatively close future, though.&lt;/p&gt;

&lt;p&gt;So, we went ahead with option 2. Among the questions at hand:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;is it feasible?&lt;/li&gt;
&lt;li&gt;would such a device need an Internet connection?&lt;/li&gt;
&lt;li&gt;how much CPU/memory would the device need?&lt;/li&gt;
&lt;li&gt;what accuracy could we expect?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;results-1&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;After a few weeks of coding, we managed to produce a pretty nice prototype,
using a Raspberry Pi, OpenCV and Python. In this limited span of time, we did
not seriously attempt to resolve issues such as shaky cameras, but otherwise,
the prototype worked nicely. Recording or finding an object in the database was
nearly instantaneous (for a database of a dozen recorded objects only), without
an internet connection, and accuracy was good (we did not attempt to quantify it).&lt;/p&gt;

&lt;p&gt;We needed to come up with a method to let the camera differentiate the object
from its background. In the absence of sophisticated stereoscopic cameras, we
used two tricks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;ask the user to show the object, then hide it; or&lt;/li&gt;
&lt;li&gt;ask the user to shake the object in from of the camera.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We tested some of the prototypes with real users, got good feedback
and figured that we could iterate from there. Our prototypes cost ~$50 to build,
a far cry from the $1,500+ we found for other devices for vision-impaired users.&lt;/p&gt;

&lt;p&gt;Also, other tests with a few fully blind users confirmed that aiming with a
camera was not a problem. There were difficulties with judging distance or
light, of course, but we expected that these could be mitigated by judicious
algorithms.&lt;/p&gt;

&lt;p&gt;So, we decided to proceed from this prototype.&lt;/p&gt;

&lt;h1 id=&#34;form-factor&#34;&gt;Form factor&lt;/h1&gt;

&lt;p&gt;Once we were reasonably convinced that we could come up with a technology to
make the device work, we started working on the form factor. We came up with
several ideas:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a camera in a necklace;&lt;/li&gt;
&lt;li&gt;a camera attached to the user&amp;rsquo;s glasses;&lt;/li&gt;
&lt;li&gt;a camera that could attach to many places, including a supermarket trolley.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unfortunately, the project was canceled before we could make much headway in
the form factor.&lt;/p&gt;

&lt;p&gt;An important lesson, though, is that going from design to industrial prototype
takes lots of time. I do not have hard numbers, as we didn&amp;rsquo;t reach that stage,
but my personal estimate puts this around &lt;strong&gt;6 months&lt;/strong&gt;. Which means that we
needed a way to test our code and our form factor prototypes with real users.&lt;/p&gt;

&lt;p&gt;We did not get the opportunity to test the form factor, although &lt;a href=&#34;how-to-survive.html&#34;&gt;I discussed in
another post&lt;/a&gt; some of the techniques that we could have
used for this purpose.&lt;/p&gt;

&lt;h1 id=&#34;app&#34;&gt;App&lt;/h1&gt;

&lt;p&gt;It would have been unrealistic to wait for the industrial prototypes before
proceeding, so we came up with an idea to test our code in parallel with the
external and industrial design.&lt;/p&gt;

&lt;p&gt;The idea was to port our algorithms to a device that a sufficiently large number
of our user base already owned. While developing for Android would have been
nice, our statistics (US only) showed almost all vision-impaired users who owned
a smartphone were using iPhones, so we did not have much of a choice. We decided
to develop an application for iPhone, release it first on Apple&amp;rsquo;s TestFlight
(for alpha-testers), then the App Store.&lt;/p&gt;

&lt;p&gt;Additionally, we expected that some users who already owned an iPhone could be
interested in the application, even without an optimal form factor. For other users, we still intended to provide an
inexpensive full-featured device.&lt;/p&gt;

&lt;h2 id=&#34;results-2&#34;&gt;Results&lt;/h2&gt;

&lt;p&gt;We managed to port our code relatively easily from Linux to iOS but the project
was canceled while we were still 1-2 weeks away from a first pre-alpha release,
an estimated 4-6 weeks before we could go beta.&lt;/p&gt;

&lt;p&gt;Performance was excellent, which is no surprise as iPhones are largely more
powerful than Raspberry Pis.&lt;/p&gt;

&lt;p&gt;While population numbers make it clear that iOS was the right platform for early
testing and would be the only reasonable target for a smartphone-only application
for vision-impaired people &lt;em&gt;in the US&lt;/em&gt;, we had a terrible experience developing
for iPhone.&lt;/p&gt;

&lt;p&gt;Since we needed to interact with a C++ library for computer vision, we needed
to code in three languages: C++ (for the computer vision), Swift (for the UX)
and Obj-C++ (the unnatural grandchild of C), with two different threading models
and three different memory management paradigms (C, C++, Swift/Obj-C). Getting
to a stage at which we could perform callbacks from C++ to Swift took some
serious digging and understanding Obj-C++&amp;rsquo;s very &lt;em&gt;interesting&lt;/em&gt; memory model.&lt;/p&gt;

&lt;p&gt;The signature mechanism seems to be designed largely to get developers to pay
(either $99 per developer or $199 per team), which is quite adverse to both
open-source contributions and to continuous integration. It also makes it
hard/impossible to distribute binaries for testing, even to your non-developer
teammates.&lt;/p&gt;

&lt;p&gt;Also, the command-line tools offer now reliable way to launch unit/integration
tests, plus they have breaking syntax changes – these two points contribute to
making continuous integration needlessly difficult.&lt;/p&gt;

&lt;h1 id=&#34;not-tested-crowdsourcing-help&#34;&gt;Not tested: Crowdsourcing help&lt;/h1&gt;

&lt;p&gt;One of the features we were considering was a way to crowdsource requests for
assistance. Consider a vision-impaired user with a question that cannot be
easily answered by a computer: &amp;ldquo;My arm hurts, is there anything strange on it?&amp;rdquo;,
&amp;ldquo;Does this dress match this hat?&amp;rdquo;, etc. How can we put the user in touch with
someone willing to help?&lt;/p&gt;

&lt;p&gt;Broadly speaking, our idea was the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;extend the camera device and application with the ability to record a question;&lt;/li&gt;
&lt;li&gt;develop a web service to gather the questions, their answers, moderation, meta-moderation, &amp;hellip;;&lt;/li&gt;
&lt;li&gt;develop a web application front-end to the web service;&lt;/li&gt;
&lt;li&gt;wrap this web application as smartphone apps and browser add-ons for all major
browsers, designed so that a user could spend ~5-20 seconds helping whenever they
felt like it.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This left many issues unanswered, such as how to avoid spamming users with
inappropriate content, how to handle privacy (&amp;ldquo;I have photographed my credit
card by accident&amp;rdquo;), etc.&lt;/p&gt;

&lt;p&gt;We only had time for the first paper explorations of the topic, but we
believed that such an open-source crowdsourcing platform for requesting
5-20 seconds help could have become very useful for many projects.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Things to do before you[r project] die[s]</title>
      <link>https://yoric.github.io/post/how-to-survive/</link>
      <pubDate>Fri, 17 Feb 2017 00:10:34 +0100</pubDate>
      
      <guid>https://yoric.github.io/post/how-to-survive/</guid>
      <description>&lt;p&gt;It&amp;rsquo;s no secret that all projects die, eventually, and that most projects die
before they have had a chance to have any impact. This is true of personal projects,
of open-source projects, of startup projects, of hardware projects, and more.&lt;/p&gt;

&lt;p&gt;A few days ago, &lt;a href=&#34;https://wiki.mozilla.org/Connected_Devices/Projects/Project_Lighthouse&#34;&gt;the IoT project on which I was working&lt;/a&gt;
did just that. It was funded, it was open-source, it was open-hardware, and it
still died before having the opportunity to release anything useful. In our case,
we got pretty far, didn&amp;rsquo;t run out of funds, and were a little polish away from
putting our first alpha-testers in front of a pretty advanced prototype before
a higher-level strategy pivot killed a number of projects at once.&lt;/p&gt;

&lt;p&gt;Ah, well, risks of the trade.&lt;/p&gt;

&lt;p&gt;Still, working on it, we managed to learn or come up with a few tricks
to increase the chances of surviving at least long enough to release a product.
I&amp;rsquo;ll try to summarize and explain some of these tricks here. Hopefully, if you
have a project, regardless of funding and medium, you may find some of this
summary useful.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;

&lt;h1 id=&#34;death-by-nobody-will-use-your-project&#34;&gt;Death by Nobody-will-use-your-project&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;tl;dr&lt;/p&gt;

&lt;p&gt;There are many good ways to test an idea way before the prototype is solid
enough to show it up. Use them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You have an idea. It&amp;rsquo;s certainly a great idea. Your friends and your family
probably love it. A few geeks around you are enthusiastic, it can change the world!&lt;/p&gt;

&lt;p&gt;At this stage, you are eager to start coding/designing/burning circuits/&amp;hellip;
And that&amp;rsquo;s great. By all means, if you&amp;rsquo;re enjoying yourself, please carry on.
However, before you start committing to the project anything you might regret –
leaving your day job, spending your money or your investors&amp;rsquo; – you should make
sure that your project has a chance to gain users and traction.&lt;/p&gt;

&lt;p&gt;If you come from a technological background, you may believe that you need to
put your project in front of users to get feedback, which means that you need
a pretty advanced prototype. &lt;em&gt;If you do this, you considerably increase your
chances of your project dying on you.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;So time to put your Survivor Hat.&lt;/p&gt;

&lt;p&gt;There are many ways to get user feedback while the project is in its
infancy. Doing this used to be long, expensive and – just as important for an indie
project – boring. These days, there are a few services that do it for you
and actually bring money to your project in case of success. Yes, I am talking of &lt;a href=&#34;http://kickstarter.com/&#34;&gt;Kickstarter&lt;/a&gt;, &lt;a href=&#34;http://indiegogo.com/&#34;&gt;Indiegogo&lt;/a&gt;
&amp;amp; &lt;a href=&#34;https://en.wikipedia.org/wiki/Comparison_of_crowdfunding_services&#34;&gt;co&lt;/a&gt;. If you
believed that these crowdfunding platforms were about gathering money,
welcome to the worst-kept secret of the 2010s:
they are actually marketing platforms that will let you find out &lt;em&gt;if people are
willing to spend money on your project&lt;/em&gt;. Which, in many cases, is a pretty good
measure of whether you should greenlight your own project. Just be sure to be
honest regarding the current state of your project and how the money will be
used. If you lie, even with the best of intentions, your funders will eventually
find out and they will be legitimately angry.&lt;/p&gt;

&lt;p&gt;An alternative, depending on what you&amp;rsquo;re doing, is to open a website and start
taking pre-orders (morally dubious), start letting interested parties subscribe
to an announcement mailing-list (better), or get creative. I have even seen weirder variants in
which non-technical founders started a fairly sophisticated online service
before they even hired a single developer – by hiring underpaid students to actually do
the work of the machine, at least long enough to determine whether users would
actually use them (neat trick, morally&amp;hellip; weird). I have also seen at least one startup-turned-internet-giant who implemented only a subset of the promised
features and measured how many users clicked on buttons that were not
implemented yet (not a trick I like, but it worked for them).&lt;/p&gt;

&lt;p&gt;Note that all these techniques help you determine your potential end users.
Unless your end users are developers, that&amp;rsquo;s entirely orthogonal to getting
people to take part in the development of your project.&lt;/p&gt;

&lt;p&gt;This list is clearly not exhaustive. Now that you have your Survivor Hat on,
you should be able to come up alternatives if you need to.&lt;/p&gt;

&lt;p&gt;Regardless of the technique you use, spending a few days of work to measuring
your potential audience can save you from disastrous choices for your career
(or savings, or startup, &amp;hellip;). As a bonus, putting together some communication
will also serve to put your own thoughts in order, decide of your priorities,
get feedback from your future users. Also, if you need money, it can serve to
fund you (crowdfunding) and/or to convince investors.&lt;/p&gt;

&lt;h1 id=&#34;death-by-tests-need-users-need-tests&#34;&gt;Death by Tests-need-users-need-tests&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;tl;dr&lt;/p&gt;

&lt;p&gt;There are many good ways to test algorithms, UX, accessibility, &amp;hellip; before the
prototype is solid enough to show it up. Use them.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At this stage, I&amp;rsquo;m assuming that you have identified your user population and
the conditions in which your project will be used. You will, of course, need
to adapt – you will need different testers for an OCR-for-the-blind or for a
superbowl-adblocker. For narrative purposes, though, I&amp;rsquo;ll pretend that your testers
somehow lurk in your hallway.&lt;/p&gt;

&lt;p&gt;So, your users are interested, your project is going full speed ahead. Great.
But, like it or not, full speed ahead is often still too slow with regard
to testing. Why? Because
just as code doesn&amp;rsquo;t work until it has been {unit, integration}-tested, a
product doesn&amp;rsquo;t work until it has been user-tested. Again, if you are thinking
as a developer, you will be tempted to wait until you have an advanced prototype.
But, if you are thinking as a developer, you also know that you don&amp;rsquo;t wait until
the project is nearly complete to start adding {unit, integration} tests. This
is doubly true if your project involves hardware – just building the hardware
in the correct form factor can take months that you typically do not have.&lt;/p&gt;

&lt;p&gt;So put on your Survivor Hat and find ways to run user tests
on a prototype that doesn&amp;rsquo;t exist yet.&lt;/p&gt;

&lt;p&gt;UX can be tested on mockups. If it&amp;rsquo;s a GUI, a text interface or a braille
interface, draw it using any mockup tool, or
even on paper, grab some people in the hallway, see if they can guess how to
go from &amp;ldquo;I want to do this&amp;rdquo; to &amp;ldquo;I&amp;rsquo;m clicking the right button&amp;rdquo;. If it&amp;rsquo;s a
sound/voice UI, hide your mockup, grab some more people in the hallway, and
roleplay the speech recognition/text-to-speech engine, etc. Even wearable IoT
devices can be mocked up using wire, velcro, a smartphone and some bluetooth
accessories.&lt;/p&gt;

&lt;p&gt;Similarly, you can user-test algorithms. Add a mockup UX on top of it – again,
it can be as simple as paper – grab some people in the hallway and run the
algorithm. If the algorithm is simple enough, you can again roleplay it. If it is
more complex, turn it into a command-line/REPL toplevel and run it behind the
scenes.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s go further. Are you building an IoT device? If you are lucky, you can
quickly rig together a prototype running your code on top of a smartphone,
smart tv, alexa or any existing device that your future users already own –
and if no smart device has all the capabilities you need for testing your code,
maybe a Bluetooth thingy can provide the missing features. Or perhaps you can
prototype your software application as a browser add-on. Or detach a single
feature and turn it into an App.
Suddenly, instead of having access to a handful of potential alpha-testers,
you can put your code in front of potential millions.&lt;/p&gt;

&lt;p&gt;Again, regardless of the technique you use, spending a few weeks on mockups/early
prototypes will let you perform user test in parallel with the unit and integration
tests that you already run continuously. Much as unit/integration tests, user
tests will let you fix issues before they kill you. Hey, sometimes, &lt;a href=&#34;https://vr.google.com/cardboard/&#34;&gt;a prototype&lt;/a&gt;
can even &lt;em&gt;become&lt;/em&gt; &lt;a href=&#34;https://vr.google.com/daydream/&#34;&gt;a product&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&#34;now-what&#34;&gt;Now what?&lt;/h1&gt;

&lt;p&gt;Well, hopefully, this blog post gave you a few ideas. Now, put on your
Survivor Hat and start thinking about ways to make your project survive until
it has a chance to have an impact!&lt;/p&gt;

&lt;p&gt;As a side-note, while I came up with the &amp;ldquo;Survivor Hat&amp;rdquo; as a narrative to
explain some of the techniques above to fellow developers and while we came up
with some of the ideas above mostly independently during Project Lighthouse,
this blog entry is largely about some of the &amp;ldquo;Lean Startup&amp;rdquo; concepts. So, if you
are interested you should find it easy to read more on the general idea.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Migrating again</title>
      <link>https://yoric.github.io/post/migration-2/</link>
      <pubDate>Fri, 17 Feb 2017 00:07:21 +0100</pubDate>
      
      <guid>https://yoric.github.io/post/migration-2/</guid>
      <description>&lt;p&gt;While Hubpress is a cool project, I had no idea of what it could be doing with
my password, so I have migrated to Hugo.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Extracting text from images is not easy (who would have guessed?)</title>
      <link>https://yoric.github.io/post/extracting-text-from-images/</link>
      <pubDate>Sun, 01 Jan 2017 23:44:19 +0100</pubDate>
      
      <guid>https://yoric.github.io/post/extracting-text-from-images/</guid>
      <description>

&lt;p&gt;At its core, Lighthouse is an idea we have been discussing in Connected Devices: can we build a device that will help people with partial or total vision
disabilities?&lt;/p&gt;

&lt;p&gt;From there, we started a number of experiments. I figured out it was time to
braindump some of them.&lt;/p&gt;

&lt;h2 id=&#34;our-problem&#34;&gt;Our problem&lt;/h2&gt;

&lt;p&gt;Consider the following example:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/10190/18631303/80cce95a-7e71-11e6-8a4e-bed542e6e7bb.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;How do we get from this beautiful picture of Mozilla&amp;rsquo;s Paris office to the text &amp;ldquo;PRIDE and PREJUDICE&amp;rdquo;, &amp;ldquo;Jane Austen&amp;rdquo;, &amp;ldquo;Great Books&amp;rdquo;, &amp;ldquo;Great Prices&amp;rdquo;, &amp;ldquo;$9.99&amp;rdquo;, &amp;ldquo;Super livres&amp;rdquo;, &amp;ldquo;Super prix&amp;rdquo;? That&amp;rsquo;s Canadian dollars, if you wonder.&lt;/p&gt;

&lt;p&gt;Too easy? What about this one?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/10190/18631577/13481e2a-7e73-11e6-9057-72aa048d7040.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Or that one?&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cloud.githubusercontent.com/assets/10190/18631784/45764bd2-7e74-11e6-9b2c-5ef823fd129a.jpeg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Not very easy to read, right? Now, remember one thing: these pictures were taken by people who can use a camera correctly because, well, they can see the image. If we want to build a device that will be useful for blind or vision-impaired users, we need a tool that can perform text recognition on images that are taken in much worse conditions or light or angle – or at least, we must be able to tell the user what&amp;rsquo;s the problem with the image.&lt;/p&gt;

&lt;p&gt;Actually, extracting text from an image is a Hard Problem &amp;trade;. Sufficiently hard that there are tracks in research conferences that deal with this kind of problem. But, surely, in this age of Deep Learning, there must be a Magic Cloud API &amp;trade; that can do that, right?&lt;/p&gt;

&lt;h2 id=&#34;looking-for-a-cloud-api&#34;&gt;Looking for a Cloud API&lt;/h2&gt;

&lt;p&gt;So, we set out to benchmarkg existing APIs that claim that they can perform text recognition on images. This includes Google&amp;rsquo;s APIs, IBM/AlchemyVision&amp;rsquo;s APIs, CloudSight&amp;rsquo;s APIs, Microsoft&amp;rsquo;s Vision APIs.&lt;/p&gt;

&lt;p&gt;If you are curious, I invite you to try with the three samples above. The results are limited. In some cases, we get acceptable recognition. In most cases, the APIs just can&amp;rsquo;t help.&lt;/p&gt;

&lt;p&gt;To make things worse, these Magic Cloud APIs are, of course, not free. Using them comes at a cost, in terms of device bandwith, data plan, and actual per-request dollars.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Bottom line&lt;/em&gt; These Magic Cloud APIs are acceptable for a demo, but are far from sufficient if we want to build a tool useful for an actual blind person.&lt;/p&gt;

&lt;h2 id=&#34;looking-for-an-offline-api&#34;&gt;Looking for an offline API&lt;/h2&gt;

&lt;p&gt;These days, the recommended OCR library in the open-source world seems to be Tesseract. Testing it against any of the images above yields strictly nothing usable.&lt;/p&gt;

&lt;p&gt;This is not a surprise: Tesseract is fine-tuned for use in a scanner. Give it black letters, white paper, no textures, and a simple layout, and Tesseract will return text with an acceptable quality. Give it anything else and Tesseract will assume that it&amp;rsquo;s black letters, white paper, no textures and a simple layout, and will attempt to interpret every single line, every single intersection as a letter. And fail.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Bottom line&lt;/em&gt; On its own, Tesseract is not usable.&lt;/p&gt;

&lt;h2 id=&#34;looking-for-a-way-out&#34;&gt;Looking for a way out&lt;/h2&gt;

&lt;p&gt;To summarize, I believe that we have the following problems:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Magic Cloud APIs attempt to solve all the issues in the world at once and are consequently great at nothing;&lt;/li&gt;
&lt;li&gt;Tesseract is too specific to a single use case which is not ours, and consequently not usable as-is for our project.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&#34;working-around-the-limitations-of-tesseract&#34;&gt;Working around the limitations of Tesseract&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s start with problem 2., because there are many options to solve it. If Tesseract cannot be used on unclean text, we &lt;em&gt;just&lt;/em&gt; need to clean up the text, using well-known or not-so-well-known Computer Vision algorithms, before feeding it to Tesseract. A quick look at Stack Overflow indicates that we are hardly the first project attempting to do this and that there are a number of solutions that &lt;em&gt;should&lt;/em&gt; work. Sadly, not much feedback on whether they &lt;em&gt;actually&lt;/em&gt; work, but hey, that&amp;rsquo;s what experimenting is all about, right?&lt;/p&gt;

&lt;p&gt;If we succeed, we will have at hand a super-Tesseract, well-adapted to &lt;em&gt;some categories of images&lt;/em&gt;. A super-Tesseract that we can further train to adapt it to the images in which we are interested. Say, text recognition on packaging, with the lighting conditions available in a supermarket.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We have a plan&lt;/em&gt; we don&amp;rsquo;t need to build a full OCR engine, we just need a flexible text cleanup/extraction tool.&lt;/p&gt;

&lt;h3 id=&#34;working-around-poor-lights-angle&#34;&gt;Working around poor lights, angle, &amp;hellip;&lt;/h3&gt;

&lt;p&gt;Let&amp;rsquo;s be pessimistic and assume that, despite our best efforts, our super-Tesseract will not be able to handle poor light, poor angle, etc. any better than the Magic Cloud APIs.&lt;/p&gt;

&lt;p&gt;Super-Tesseract will still have a few aces up its metaphoric sleeve:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;If we run it locally, on the user&amp;rsquo;s device, it is not limited by bandwidth, data plan or per-request cost;&lt;/li&gt;
&lt;li&gt;While performing actual text recognition is relatively slow on a smartphone-style device, we have hopes that performing text detection and cleanup can be made quite fast.&lt;/li&gt;
&lt;li&gt;As part of super-Tesseract, we can build algorithms that can actually determine whether the angle is wrong, or the lightning is bad, etc. All sorts of information that the Magic Cloud APIs do not give us.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Put together, these two aces open several possibilities:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;We can afford to perform at least the first few steps of super-Tesseract &lt;em&gt;on videos&lt;/em&gt;, rather than on a single image, which will give us more samples from which to extract clean text.&lt;/li&gt;
&lt;li&gt;We can afford to perform these same steps several times, using several different filters and parameters, to offset the light, angle, etc.&lt;/li&gt;
&lt;li&gt;We can provide feedback for the users, actually telling them what&amp;rsquo;s wrong, if they need to turn the cereal bar, or move the camera closer, or if the text is just simply unreadable because of too many textures.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Bottom line&lt;/em&gt;: We need a very fast implementation of text clean-up/extraction that can also diagnose its own problems.&lt;/p&gt;

&lt;h2 id=&#34;towards-super-tesseract&#34;&gt;Towards super-Tesseract&lt;/h2&gt;

&lt;p&gt;At this stage, my hopes are to perform the following steps:&lt;/p&gt;

&lt;p&gt;a. locating the text in an image;
b. removing everything that isn&amp;rsquo;t the text;
c. turning it if necessary to make sure that it&amp;rsquo;s horizontal;
d. now that the text is as clean as in a book, feed it to Tesseract.&lt;/p&gt;

&lt;p&gt;During the past few weeks, I have been experimenting with algorithms for these things. All the problems are tricky, but I am making progress.&lt;/p&gt;

&lt;p&gt;a. An algorithm based on &lt;a href=&#34;https://www.microsoft.com/en-us/research/publication/stroke-width-transform/[Stroke&#34;&gt;https://www.microsoft.com/en-us/research/publication/stroke-width-transform/[Stroke&lt;/a&gt; Width Transforms] (SWT) has very good results on locating the text in an image. I have tested the implementation of this algorithm in the &lt;a href=&#34;https://github.com/liuliu/ccv[CCV&#34;&gt;https://github.com/liuliu/ccv[CCV&lt;/a&gt;] library and it has very good performance, both in terms of speed and in terms of results. So far, so good. As I am prototyping in Rust, I have published &lt;a href=&#34;https://github.com/Yoric/ccv.rs[very&#34;&gt;https://github.com/Yoric/ccv.rs[very&lt;/a&gt; early bindings] – feel free to use them if you want to toy with this algorithm.&lt;/p&gt;

&lt;p&gt;b. I have tested several algorithms for cleaning up everything that isn&amp;rsquo;t text. Unfortunately, so far, these algorithms perform quite poorly, both in terms of speed and accuracy. Looking harder at SWT, I believe that this algorithm can be extended to perform the cleanup. I am currently working such an extension, but it might be a few weeks before I can seriously test it.&lt;/p&gt;

&lt;p&gt;c. An algorithm called Deskewing is rumored to have good results. I have located an implementation of this as part of a library called
&lt;a href=&#34;http://www.leptonica.org/[Leptonica&#34;&gt;http://www.leptonica.org/[Leptonica&lt;/a&gt;]. The implementation is very fast, but it doesn&amp;rsquo;t work too well on my samples. I need to investigate this further. It may be a problem with my samples, which are not sufficiently cleaned up at this stage, or with the algorithm, or even with me misunderstanding the results of the algorithm. Again, I have published &lt;a href=&#34;https://github.com/Yoric/lepton.rs[minimal&#34;&gt;https://github.com/Yoric/lepton.rs[minimal&lt;/a&gt; Rust bindings for Leptonica], to simplify experimentation.&lt;/p&gt;

&lt;h2 id=&#34;things-not-text&#34;&gt;Things not text&lt;/h2&gt;

&lt;p&gt;Byproduct of the study: there are several very important pieces of
information on boxes and clothes that would be useful for blind or vision-impaired users but that most likely cannot be treated by OCR.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m thinking of logos (e.g. no amount of OCR will recognize
Coca Cola, Kellogs, &amp;hellip;) and other symbols (e.g. non-recycle plastic,
toxicity warning, hand wash only, &amp;hellip;)&lt;/p&gt;

&lt;p&gt;At some point, we will certainly want to recognize them. I suspect that neural networks/deep learning would be pretty good for that purpose, but I have no idea how much effort that would take. I also suspect that Magic Cloud APIs already handle these quite nicely already, so at least for a prototype, it might be useful to use these as a baseline.&lt;/p&gt;

&lt;h1 id=&#34;commenting&#34;&gt;Commenting&lt;/h1&gt;

&lt;p&gt;You can leave comments &lt;a href=&#34;https://github.com/Yoric/yoric.github.io/issues/1[on&#34;&gt;https://github.com/Yoric/yoric.github.io/issues/1[on&lt;/a&gt; github] or using Disqus below. I haven&amp;rsquo;t decided what&amp;rsquo;s best yet.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Blog migration in progress</title>
      <link>https://yoric.github.io/post/migration/</link>
      <pubDate>Thu, 01 Dec 2016 23:42:19 +0100</pubDate>
      
      <guid>https://yoric.github.io/post/migration/</guid>
      <description>&lt;p&gt;I am in the process of migrating to github.io. You can find my previous blog
&lt;a href=&#34;http://dutherenverseauborddelatable.wordpress.com/&#34;&gt;on Wordpress&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>